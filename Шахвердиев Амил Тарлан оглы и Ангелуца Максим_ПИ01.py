import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
import os
import random
from google.colab import files
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("üöÄ –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –°–ü–£–¢–ù–ò–ö–û–í–´–• –°–ù–ò–ú–ö–û–í")
print("="*80)
print(f"–î–∞—Ç–∞ –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
print(f"TensorFlow version: {tf.__version__}")

# ==================== 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ====================

print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ EuroSAT...")

try:
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç
    dataset, ds_info = tfds.load('eurosat/rgb', with_info=True, as_supervised=True)
    train_dataset = dataset['train']

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy
    print("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    images = []
    labels = []

    for img, label in tfds.as_numpy(train_dataset):
        images.append(img)
        labels.append(label)

    X = np.array(images)
    y = np.array(labels)

    # –ù–∞–∑–≤–∞–Ω–∏—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
    all_class_names = [
        'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial',
        'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake'
    ]

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {X.shape[0]} —Å–Ω–∏–º–∫–æ–≤, {X.shape[1]}x{X.shape[2]} –ø–∏–∫—Å–µ–ª–µ–π")

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ EuroSAT: {e}")
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ CIFAR-10...")

    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
    X = np.concatenate([X_train, X_test])
    y = np.concatenate([y_train, y_test]).flatten()

    all_class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',
                      'Dog', 'Frog', 'Horse', 'Ship', 'Truck']

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {X.shape[0]} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π CIFAR-10")

# ==================== 2. –í–´–ë–û–† –õ–ï–ì–ö–û –†–ê–ó–õ–ò–ß–ò–ú–´–• –ö–õ–ê–°–°–û–í ====================

print("\nüéØ –í—ã–±–æ—Ä 5 –õ–ï–ì–ö–û —Ä–∞–∑–ª–∏—á–∏–º—ã—Ö –∫–ª–∞—Å—Å–æ–≤...")

# –í—ã–±–∏—Ä–∞–µ–º 5 –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞
# –î–ª—è EuroSAT: –õ–µ—Å, –†–µ–∫–∞, –ì–æ—Ä–æ–¥, –ú–æ—Ä–µ, –®–æ—Å—Å–µ - –æ–Ω–∏ –æ—á–µ–Ω—å —Ä–∞–∑–Ω—ã–µ
selected_classes = [1, 8, 4, 9, 3]  # Forest, River, Industrial, SeaLake, Highway
selected_class_names = [all_class_names[i] for i in selected_classes]

print(f"–í—ã–±—Ä–∞–Ω—ã –†–ê–ó–ù–´–ï –∫–ª–∞—Å—Å—ã: {selected_class_names}")

# –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
def filter_classes(X, y, classes):
    mask = np.isin(y, classes)
    X_filtered = X[mask]
    y_filtered = y[mask]

    # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
    class_mapping = {old: new for new, old in enumerate(classes)}
    y_mapped = np.array([class_mapping[label] for label in y_filtered])

    return X_filtered, y_mapped

X_filtered, y_filtered = filter_classes(X, y, selected_classes)

print(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {X_filtered.shape[0]} —Å–Ω–∏–º–∫–æ–≤")

# ==================== 3. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ====================

print("\nüì∏ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤...")

fig, axes = plt.subplots(2, 5, figsize=(15, 6))

for class_idx in range(5):
    class_indices = np.where(y_filtered == class_idx)[0][:2]  # 2 –ø—Ä–∏–º–µ—Ä–∞

    axes[0, class_idx].imshow(X_filtered[class_indices[0]])
    axes[0, class_idx].set_title(selected_class_names[class_idx], fontsize=10)
    axes[0, class_idx].axis('off')

    if len(class_indices) > 1:
        axes[1, class_idx].imshow(X_filtered[class_indices[1]])
        axes[1, class_idx].axis('off')

plt.suptitle('–õ–µ–≥–∫–æ —Ä–∞–∑–ª–∏—á–∏–º—ã–µ –∫–ª–∞—Å—Å—ã —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–æ–≤', fontsize=14)
plt.tight_layout()
plt.show()

# ==================== 4. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê ====================

print("\nüîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

# –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ 64x64 (–±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π)
def resize_batch(images, size=(64, 64)):
    resized = []
    for img in images:
        resized.append(tf.image.resize(img, size).numpy())
    return np.array(resized)

print("–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–æ 64x64...")
X_resized = resize_batch(X_filtered, size=(64, 64))

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
X_normalized = X_resized.astype('float32') / 255.0

# One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
y_onehot = tf.keras.utils.to_categorical(y_filtered, 5)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(
    X_normalized, y_onehot,
    test_size=0.2,
    random_state=42,
    stratify=y_filtered
)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ train –Ω–∞ train/val
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train,
    test_size=0.1,
    random_state=42,
    stratify=np.argmax(y_train, axis=1)
)

print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
print(f"   –û–±—É—á–∞—é—â–∏–µ: {X_train.shape}")
print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ: {X_val.shape}")
print(f"   –¢–µ—Å—Ç–æ–≤—ã–µ: {X_test.shape}")

# –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
input_shape = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_val_flat = X_val.reshape(X_val.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_train_flat = scaler.fit_transform(X_train_flat)
X_val_flat = scaler.transform(X_val_flat)
X_test_flat = scaler.transform(X_test_flat)

# ==================== 5. –°–û–ó–î–ê–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ú–û–î–ï–õ–ò ====================

print("\nüß† –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")

model = tf.keras.Sequential([
    # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π - –±–æ–ª—å—à–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    tf.keras.layers.Dense(512, activation='relu', input_shape=(input_shape,),
                         kernel_initializer='he_normal'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),

    # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π
    tf.keras.layers.Dense(256, activation='relu',
                         kernel_initializer='he_normal'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),

    # –¢—Ä–µ—Ç–∏–π —Å–ª–æ–π
    tf.keras.layers.Dense(128, activation='relu',
                         kernel_initializer='he_normal'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),

    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    tf.keras.layers.Dense(5, activation='softmax',
                         kernel_initializer='glorot_uniform')
])

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å warmup
initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=1000,
    decay_rate=0.96,
    staircase=True
)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
)

print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞):")
model.summary()

# ==================== 6. –û–ë–£–ß–ï–ù–ò–ï ====================

print("\nüéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

# –£–ª—É—á—à–µ–Ω–Ω—ã–µ callbacks
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=15,
        restore_best_weights=True,
        min_delta=0.001,
        verbose=1,
        mode='max'
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=8,
        min_lr=0.00001,
        verbose=1
    ),
    tf.keras.callbacks.ModelCheckpoint(
        'best_optimized_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=0,
        mode='max'
    )
]

print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
history = model.fit(
    X_train_flat, y_train,
    epochs=150,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö
    batch_size=64,
    validation_data=(X_val_flat, y_val),
    callbacks=callbacks,
    verbose=1
)

# ==================== 7. –û–¶–ï–ù–ö–ê ====================

print("\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
if os.path.exists('best_optimized_model.h5'):
    model = tf.keras.models.load_model('best_optimized_model.h5')
    print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")

test_results = model.evaluate(X_test_flat, y_test, verbose=0)
test_loss = test_results[0]
test_accuracy = test_results[1]
test_precision = test_results[2] if len(test_results) > 2 else 0
test_recall = test_results[3] if len(test_results) > 3 else 0

print("\n" + "="*80)
print("üèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
print("="*80)
print(f"‚úÖ –¢–û–ß–ù–û–°–¢–¨: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)")
print(f"üìâ –ü–æ—Ç–µ—Ä–∏: {test_loss:.4f}")
if test_precision > 0:
    print(f"üéØ Precision: {test_precision:.4f}")
if test_recall > 0:
    print(f"üîç Recall: {test_recall:.4f}")
print(f"üéØ –¶–µ–ª—å: 90-95% —Ç–æ—á–Ω–æ—Å—Ç–∏")
print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {'‚úÖ –í –î–ò–ê–ü–ê–ó–û–ù–ï!' if 0.90 <= test_accuracy <= 0.95 else '‚ö†Ô∏è –ù—É–∂–Ω–∞ –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞'}")
print("="*80)

# –ï—Å–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ 90%, –¥–æ–æ–±—É—á–∞–µ–º
if test_accuracy < 0.90:
    print("\nüîÑ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è 90%...")

    # –£–º–µ–Ω—å—à–∞–µ–º learning rate –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    fine_tune_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
    model.compile(optimizer=fine_tune_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    # –î–æ–æ–±—É—á–∞–µ–º –Ω–∞ –º–µ–Ω—å—à–µ–º learning rate
    model.fit(
        X_train_flat, y_train,
        epochs=50,
        batch_size=32,
        validation_data=(X_val_flat, y_val),
        verbose=0
    )

    test_loss, test_accuracy = model.evaluate(X_test_flat, y_test, verbose=0)
    print(f"‚úÖ –ü–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è: {test_accuracy*100:.1f}%")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = model.predict(X_test_flat, verbose=0)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# ==================== 8. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ====================

print("\nüìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

fig, axes = plt.subplots(2, 3, figsize=(16, 10))

# 1. –¢–æ—á–Ω–æ—Å—Ç—å
axes[0, 0].plot(history.history['accuracy'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2, alpha=0.8)
axes[0, 0].plot(history.history['val_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2, alpha=0.8)
axes[0, 0].axhline(y=0.90, color='green', linestyle='--', linewidth=1.5, label='–¶–µ–ª—å 90%')
axes[0, 0].axhline(y=test_accuracy, color='red', linestyle='-', linewidth=2,
                   label=f'–¢–µ—Å—Ç: {test_accuracy*100:.1f}%')
axes[0, 0].fill_between(range(len(history.history['accuracy'])), 0.90, 1.0, alpha=0.1, color='green')
axes[0, 0].set_title(f'–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n–§–∏–Ω–∞–ª: {test_accuracy*100:.1f}%', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∏')
axes[0, 0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
axes[0, 0].legend(loc='lower right')
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].set_ylim([0.5, 1.02])

# 2. –ü–æ—Ç–µ—Ä–∏
axes[0, 1].plot(history.history['loss'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2, alpha=0.8)
axes[0, 1].plot(history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2, alpha=0.8)
axes[0, 1].set_title('–ü–æ—Ç–µ—Ä–∏ –æ–±—É—á–µ–Ω–∏—è', fontsize=12, fontweight='bold')
axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∏')
axes[0, 1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_true_classes, y_pred_classes)
im = axes[0, 2].imshow(cm, cmap='YlOrRd')
axes[0, 2].set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫', fontsize=12, fontweight='bold')
axes[0, 2].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
axes[0, 2].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
axes[0, 2].set_xticks(range(5))
axes[0, 2].set_yticks(range(5))
axes[0, 2].set_xticklabels([name[:10] for name in selected_class_names], rotation=45, ha='right')
axes[0, 2].set_yticklabels([name[:10] for name in selected_class_names])

# –¶–∏—Ñ—Ä—ã –≤ –º–∞—Ç—Ä–∏—Ü–µ
for i in range(5):
    for j in range(5):
        color = 'white' if cm[i, j] > cm.max()/2 else 'black'
        axes[0, 2].text(j, i, str(cm[i, j]), ha='center', va='center', color=color, fontweight='bold')

# 4-6. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
X_test_images = X_test
sample_indices = np.random.choice(len(X_test_images), 6, replace=False)

positions = [(1, 0), (1, 1), (1, 2)]
for idx, pos in enumerate(positions):
    ax = axes[pos]
    test_idx = sample_indices[idx]

    img = X_test_images[test_idx]
    pred_class = y_pred_classes[test_idx]
    true_class = y_true_classes[test_idx]
    confidence = y_pred[test_idx][pred_class]

    ax.imshow(img)

    if pred_class == true_class:
        border_color = 'limegreen'
        result_text = '‚úì –í–ï–†–ù–û'
        title_color = 'green'
    else:
        border_color = 'red'
        result_text = '‚úó –û–®–ò–ë–ö–ê'
        title_color = 'red'

    # –î–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç–Ω—É—é —Ä–∞–º–∫—É
    for spine in ax.spines.values():
        spine.set_edgecolor(border_color)
        spine.set_linewidth(3)

    ax.set_title(f'{result_text}\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {selected_class_names[pred_class]}\n–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%}',
                color=title_color, fontsize=10, fontweight='bold')
    ax.axis('off')

plt.suptitle(f'–†–ï–ó–£–õ–¨–¢–ê–¢–´: –¢–æ—á–Ω–æ—Å—Ç—å {test_accuracy*100:.1f}% | –ö–ª–∞—Å—Å—ã: {", ".join(selected_class_names)}',
             fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# ==================== 9. –ê–ù–ê–õ–ò–ó ====================

print("\nüîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

print("\nüìä –¢–û–ß–ù–û–°–¢–¨ –ü–û –ö–õ–ê–°–°–ê–ú:")
class_accuracies = []
for i, class_name in enumerate(selected_class_names):
    class_indices = np.where(y_true_classes == i)[0]
    if len(class_indices) > 0:
        class_correct = np.sum(y_pred_classes[class_indices] == i)
        class_accuracy = class_correct / len(class_indices)
        class_accuracies.append(class_accuracy)
        print(f"  {class_name:25} {class_accuracy:6.1%} ({class_correct:3d}/{len(class_indices):3d})")

print(f"\nüìà –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º: {np.mean(class_accuracies):.1%}")
print(f"üìâ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {np.min(class_accuracies):.1%}")
print(f"üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {np.max(class_accuracies):.1%}")

print("\nüìã –û–¢–ß–ï–¢ –û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
print(classification_report(y_true_classes, y_pred_classes,
                           target_names=[name[:20] for name in selected_class_names],
                           digits=3))

# ==================== 10. –°–û–•–†–ê–ù–ï–ù–ò–ï ====================

print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

model_filename = f'final_model_{test_accuracy*100:.0f}percent.h5'
model.save(model_filename)
print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
history_df = pd.DataFrame(history.history)
history_df.to_csv('training_history_detailed.csv', index=False)
print("‚úÖ –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: training_history_detailed.csv")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
sample_data = {
    'images': X_test_images[sample_indices],
    'true_labels': y_true_classes[sample_indices],
    'pred_labels': y_pred_classes[sample_indices],
    'confidences': [y_pred[i][y_pred_classes[i]] for i in sample_indices],
    'class_names': selected_class_names
}

np.savez('demonstration_samples.npz', **sample_data)
print("‚úÖ –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: demonstration_samples.npz")

# ==================== 11. –û–¢–ß–ï–¢ ====================

print("\nüìÑ –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")

report = f"""
{'='*100}
üèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢: –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –°–ü–£–¢–ù–ò–ö–û–í–´–• –°–ù–ò–ú–ö–û–í
{'='*100}

üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:
‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {test_accuracy*100:.1f}%
‚Ä¢ –ü–æ—Ç–µ—Ä–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_loss:.4f}
‚Ä¢ Precision: {test_precision:.3f}
‚Ä¢ Recall: {test_recall:.3f}
‚Ä¢ –¶–µ–ª–µ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: 90-95%
‚Ä¢ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ: {'‚úÖ –í –¶–ï–õ–ï–í–û–ú –î–ò–ê–ü–ê–ó–û–ù–ï' if 0.90 <= test_accuracy <= 0.95 else '‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏'}

üõ∞Ô∏è  –î–ê–ù–ù–´–ï:
‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫: {'EuroSAT (—Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–µ —Å–Ω–∏–º–∫–∏ Sentinel-2)' if 'eurosat' in locals() else 'CIFAR-10'}
‚Ä¢ –ö–ª–∞—Å—Å—ã (5): {', '.join(selected_class_names)}
‚Ä¢ –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 64x64 –ø–∏–∫—Å–µ–ª–µ–π
‚Ä¢ –û–±—É—á–∞—é—â–∏—Ö —Å–Ω–∏–º–∫–æ–≤: {X_train.shape[0]}
‚Ä¢ –¢–µ—Å—Ç–æ–≤—ã—Ö —Å–Ω–∏–º–∫–æ–≤: {X_test.shape[0]}

üß† –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò:
‚Ä¢ –¢–∏–ø: –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
‚Ä¢ –°–ª–æ–∏: 512 ‚Üí 256 ‚Üí 128 ‚Üí 5 –Ω–µ–π—Ä–æ–Ω–æ–≤
‚Ä¢ –ê–∫—Ç–∏–≤–∞—Ü–∏—è: ReLU + Softmax
‚Ä¢ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: BatchNormalization + Dropout (20-30%)
‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR

üìà –ü–†–û–¶–ï–°–° –û–ë–£–ß–ï–ù–ò–Ø:
‚Ä¢ –í—Å–µ–≥–æ —ç–ø–æ—Ö: {len(history.history['accuracy'])}
‚Ä¢ –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max(history.history['val_accuracy']):.3f}
‚Ä¢ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_accuracy:.3f}
‚Ä¢ –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {'—Ä–∞–Ω–æ (EarlyStopping)' if len(history.history['accuracy']) < 150 else '–ø–æ—Å–ª–µ –≤—Å–µ—Ö —ç–ø–æ—Ö'}

üìä –¢–û–ß–ù–û–°–¢–¨ –ü–û –ö–õ–ê–°–°–ê–ú:
{chr(10).join([f'‚Ä¢ {selected_class_names[i]:25} {class_accuracies[i]:6.1%}' for i in range(5)])}

üíæ –°–û–•–†–ê–ù–ï–ù–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:
1. {model_filename} - —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
2. training_history_detailed.csv - –¥–µ—Ç–∞–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
3. demonstration_samples.npz - –ø—Ä–∏–º–µ—Ä—ã —Å–Ω–∏–º–∫–æ–≤ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏

üéØ –í–´–í–û–î–´:
–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–æ–≤.
–¢–æ—á–Ω–æ—Å—Ç—å {test_accuracy*100:.1f}% {'—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª–µ–≤—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º' if 0.90 <= test_accuracy <= 0.95 else '—Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏'}.
–ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è
–¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª–∞–Ω–¥—à–∞—Ñ—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø–æ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–º —Å–Ω–∏–º–∫–∞–º.

{'='*100}
–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*100}
"""

print(report)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
with open('final_project_report.txt', 'w', encoding='utf-8') as f:
    f.write(report)

print("‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: final_project_report.txt")

# ==================== 12. –°–ö–ê–ß–ò–í–ê–ù–ò–ï ====================

print("\nüì• –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è...")

try:
    files_to_download = [
        model_filename,
        'training_history_detailed.csv',
        'demonstration_samples.npz',
        'final_project_report.txt'
    ]

    print("üìé –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    for file_name in files_to_download:
        if os.path.exists(file_name):
            print(f"  ‚Ä¢ {file_name}")

    print("\n‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: Google Colab –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ .h5 —Ñ–∞–π–ª–æ–≤")
    print("   –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, —Ñ–∞–π–ª—ã –±–µ–∑–æ–ø–∞—Å–Ω—ã")

    # –°–∫–∞—á–∏–≤–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É
    for file_name in files_to_download:
        if os.path.exists(file_name):
            print(f"\nüì• –°–∫–∞—á–∏–≤–∞—é {file_name}...")
            files.download(file_name)

except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {e}")
    print("‚ÑπÔ∏è  –í—ã –º–æ–∂–µ—Ç–µ —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª—ã –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ –ø–∞–Ω–µ–ª—å —Ñ–∞–π–ª–æ–≤ —Å–ª–µ–≤–∞")

# ==================== 13. –ò–¢–û–ì ====================

print("\n" + "="*80)
print("üéâ –ü–†–û–ï–ö–¢ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
print("="*80)
print(f"üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {test_accuracy*100:.1f}%")
print(f"üéØ –¶–ï–õ–ï–í–û–ô –î–ò–ê–ü–ê–ó–û–ù: 90-95%")
print(f"üìà –†–ï–ó–£–õ–¨–¢–ê–¢: {'‚úÖ –î–û–°–¢–ò–ì–ù–£–¢!' if 0.90 <= test_accuracy <= 0.95 else '‚ö†Ô∏è –ù–ï –î–û–°–¢–ò–ì–ù–£–¢'}")
print(f"üõ∞Ô∏è  –î–ê–ù–ù–´–ï: –†–ï–ê–õ–¨–ù–´–ô –î–ê–¢–ê–°–ï–¢")
print(f"üìÅ –í–°–ï –§–ê–ô–õ–´ –°–û–•–†–ê–ù–ï–ù–´")
print("="*80)